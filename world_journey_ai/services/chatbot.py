from __future__ import annotations

import difflib
import html
import json
import os
import unicodedata
import hashlib
import time
from typing import Dict, List, TYPE_CHECKING
import re

from .province_guides import PROVINCE_GUIDES, PROVINCE_SYNONYMS
from .guides import build_bangkok_guides_html
from .messages import MessageStore
from .enhanced_knowledge import enhanced_knowledge, PlaceKnowledge

if TYPE_CHECKING:
    from openai import OpenAI

try:
    from openai import OpenAI as OpenAIClient
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAIClient = None  # type: ignore

TRAVEL_KEYWORDS = (
    # Thai - Basic travel terms
    "‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏ó‡∏£‡∏¥‡∏õ", "‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ", "‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á",
    "‡∏ó‡∏∞‡πÄ‡∏•", "‡∏†‡∏π‡πÄ‡∏Ç‡∏≤", "‡∏ß‡∏±‡∏î", "‡∏ï‡∏•‡∏≤‡∏î", "‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà", "‡πÇ‡∏Æ‡∏°‡∏™‡πÄ‡∏ï‡∏¢‡πå", "‡∏ô‡πâ‡∏≥‡∏ï‡∏Å", "‡∏î‡∏≥‡∏ô‡πâ‡∏≥", "‡πÄ‡∏î‡∏¥‡∏ô‡∏õ‡πà‡∏≤", "‡πÄ‡∏Å‡∏≤‡∏∞",
    "‡πÑ‡∏ô‡∏ó‡πå‡∏°‡∏≤‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ï", "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥", "‡∏ä‡∏≤‡∏¢‡∏´‡∏≤‡∏î", "‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô", "‡∏™‡∏ß‡∏ô", "‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå",
    
    # Thai - Administrative divisions and location queries
    "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "‡∏≠‡∏≥‡πÄ‡∏†‡∏≠", "‡∏ï‡∏≥‡∏ö‡∏•", "‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô", "‡πÄ‡∏Ç‡∏ï", "‡πÅ‡∏Ç‡∏ß‡∏á", "‡∏¢‡πà‡∏≤‡∏ô", "‡∏ä‡∏∏‡∏°‡∏ä‡∏ô", "‡∏´‡∏°‡∏π‡πà", "‡∏ö‡πâ‡∏≤‡∏ô",
    "‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö", "‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å", "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏≠‡∏Å", "‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ",
    "‡∏°‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏à‡∏∏‡∏î‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏î‡∏µ",
    "‡∏Ñ‡∏ß‡∏£‡πÑ‡∏õ", "‡∏ô‡πà‡∏≤‡πÑ‡∏õ", "‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á", "‡πÄ‡∏î‡πá‡∏î", "‡∏î‡∏±‡∏á", "‡πÇ‡∏î‡πà‡∏á‡∏î‡∏±‡∏á", "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ‡πÜ", "‡∏ô‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°",
    
    # English - Basic travel terms  
    "travel", "trip", "itinerary", "journey", "vacation", "holiday", "beach", "mountain", "temple",
    "market", "night market", "floating market", "cafe", "coffee shop", "homestay", "waterfall",
    "snorkel", "snorkeling", "diving", "hike", "hiking", "island", "old town", "museum", "park",
    
    # English - Administrative divisions and location queries
    "province", "district", "subdistrict", "tambon", "amphoe", "village", "town", "city", "area",
    "where is", "near to", "close to", "far from", "what to see", "what to do", "recommend",
    "suggest", "tell me about", "interesting", "attractions", "tourist spots", "worth visiting",
    "should visit", "must see", "famous for", "known for", "popular", "best places",
)

TRAVEL_ONLY_MESSAGE = (
    "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞! ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Å‡∏î‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏∞ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤ ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏∞"
)

GUIDE_ONLY_MESSAGE = (
    "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞! ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏£‡∏¥‡∏õ‡πÉ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏∞ ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤ ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô"
)

# Prototype: Restrict to Samutsongkhram province only
SAMUTSONGKHRAM_ONLY_MESSAGE = (
    "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Å‡∏î‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏∞ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô:\n"
    "üèõÔ∏è ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á (‡πÇ‡∏ö‡∏™‡∏ñ‡πå‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£)\n"
    "üõ∂ ‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤\n"
    "üå≤ ‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô (‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô)\n"
    "üèõÔ∏è ‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 2\n"
    "üö£ ‡∏ö‡πâ‡∏≤‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å\n\n"
    "‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡∏π‡∏Ñ‡πà‡∏∞!"
)


CATEGORY_LABELS = [
    "‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏®‡∏±‡∏Å‡∏î‡∏¥‡πå‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
    "‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏≤‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥",
    "‡∏ï‡∏•‡∏≤‡∏î‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°",
    "‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
    "‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡∏≠/‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô",
]


class BaseAIEngine:
    """Base class for AI engines with enhanced role memory and persistent behavior"""
    
    def __init__(self, message_store: MessageStore, destinations: List[Dict[str, str]], ai_mode: str = "general") -> None:
        self._store = message_store
        self._destinations = destinations
        self._ai_mode = ai_mode  # "chat", "guide", or "general"
        self._normalized_dest_names = [self._normalize(item["name"]) for item in destinations]
        self._normalized_keywords = [self._normalize(keyword) for keyword in TRAVEL_KEYWORDS]
        
        # Initialize enhanced knowledge system
        self.enhanced_knowledge = enhanced_knowledge
        
        # Enhanced Role Memory System
        self._role_memory = {
            "personality": self._get_ai_personality(),
            "conversation_context": [],
            "user_preferences": {},
            "session_goals": [],
            "expertise_areas": self._get_expertise_areas(),
            "behavioral_guidelines": self._get_behavioral_guidelines(),
            "conversation_history_summary": "",
            "last_topics": [],
            "user_interaction_style": "adaptive"
        }
        
        # Conversation continuity tracking
        self._conversation_state = {
            "current_topic": None,
            "context_depth": 0,
            "follow_up_suggestions": [],
            "unresolved_queries": [],
            "expertise_confidence": "high"
        }
        
        # Initialize caching system for 95% accuracy
        self._response_cache: Dict[str, Dict[str, object]] = {}
        self._cache_timestamps: Dict[str, float] = {}
        self._cache_max_age = 3600  # 1 hour cache timeout
        self._cache_max_size = 1000  # Maximum cache entries
        
        # Initialize OpenAI client if available
        self._openai_client = None
        if OPENAI_AVAILABLE:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                try:
                    self._openai_client = OpenAIClient(api_key=api_key)  # type: ignore
                except Exception:
                    pass
        self._province_aliases = self._build_province_aliases()

    def _get_ai_personality(self) -> Dict[str, str]:
        """Define the AI's core personality traits and behavior patterns"""
        base_personality = {
            "name": "‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π (Nong Pla Too)",
            "role": "Expert Travel Consultant & Cultural Guide",
            "personality_traits": [
                "Enthusiastic and helpful",
                "Culturally sensitive and respectful", 
                "Detail-oriented and accurate",
                "Friendly but professional",
                "Adaptive to user communication style"
            ],
            "communication_style": "Warm, informative, and encouraging",
            "expertise_confidence": "High (95%+ accuracy standards)",
            "language_adaptation": "Mirrors user's language preference",
            "cultural_awareness": "Deep understanding of local customs and etiquette"
        }
        
        if self._ai_mode == "guide":
            base_personality.update({
                "specialized_role": "Trip Planning Specialist",
                "focus_areas": ["Itinerary planning", "Budget optimization", "Experience curation"],
                "interaction_style": "Systematic and goal-oriented"
            })
        elif self._ai_mode == "chat":
            base_personality.update({
                "specialized_role": "Conversational Travel Companion", 
                "focus_areas": ["General travel advice", "Cultural insights", "Recommendations"],
                "interaction_style": "Casual and exploratory"
            })
            
        return base_personality

    def _get_expertise_areas(self) -> List[str]:
        """Define the AI's areas of expertise for consistent role reinforcement"""
        return [
            "üåè Thailand (Expert Level): All 77 provinces, cultural nuances, hidden gems",
            "üåç Global Destinations: 8 major cities with comprehensive knowledge",
            "üèõÔ∏è Cultural Sensitivity: Local customs, etiquette, responsible travel",
            "üó∫Ô∏è Practical Planning: Transportation, accommodation, budget optimization",
            "üçú Local Cuisine: Food culture, dietary restrictions, authentic experiences",
            "üé≠ Seasonal Awareness: Weather patterns, festivals, optimal timing",
            "üí∞ Budget Management: Cost-effective strategies across price ranges",
            "üöó Transportation: Multi-modal travel planning and logistics"
        ]

    def _get_behavioral_guidelines(self) -> List[str]:
        """Define consistent behavioral patterns the AI should maintain"""
        return [
            "Always maintain friendly but professional demeanor",
            "Prioritize accuracy over impressive-sounding information",
            "Acknowledge limitations honestly when uncertain",
            "Adapt communication style to match user's preference",
            "Provide actionable, practical advice with specific details", 
            "Include cultural context and respect local customs",
            "Remember previous conversation topics and user preferences",
            "Offer alternatives and backup options for resilience",
            "Balance comprehensive information with clear, digestible presentation",
            "Encourage sustainable and responsible travel practices"
        ]

    def _update_conversation_memory(self, user_input: str, ai_response: str) -> None:
        """Update conversation memory to maintain context and continuity"""
        # Extract topics and preferences from user input
        topics = self._extract_topics(user_input)
        preferences = self._extract_preferences(user_input)
        
        # Update conversation context (keep last 10 exchanges)
        self._role_memory["conversation_context"].append({
            "user_input": user_input[:200],  # Truncate for memory efficiency
            "ai_response_summary": ai_response[:100],
            "topics": topics,
            "timestamp": time.time()
        })
        
        # Keep only recent context
        if len(self._role_memory["conversation_context"]) > 10:
            self._role_memory["conversation_context"] = self._role_memory["conversation_context"][-10:]
        
        # Update user preferences
        self._role_memory["user_preferences"].update(preferences)
        
        # Update last topics
        self._role_memory["last_topics"] = topics[-5:] if topics else self._role_memory["last_topics"]
        
        # Update conversation state
        if topics:
            self._conversation_state["current_topic"] = topics[-1]
            self._conversation_state["context_depth"] += 1

    def _extract_topics(self, text: str) -> List[str]:
        """Extract conversation topics for context tracking"""
        topics = []
        text_lower = text.lower()
        
        # Destination topics
        for dest in self._destinations:
            if dest["name"].lower() in text_lower:
                topics.append(f"destination:{dest['name']}")
        
        # Activity topics
        activity_keywords = {
            "food": ["‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏Å‡∏¥‡∏ô", "food", "restaurant", "eat"],
            "accommodation": ["‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å", "‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°", "hotel", "stay", "accommodation"],
            "transportation": ["‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á", "‡∏£‡∏ñ", "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏¥‡∏ô", "transport", "travel", "flight"],
            "culture": ["‡∏ß‡∏±‡∏î", "‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°", "‡∏õ‡∏£‡∏∞‡πÄ‡∏û‡∏ì‡∏µ", "temple", "culture", "tradition"],
            "budget": ["‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "budget", "cost", "price"]
        }
        
        for category, keywords in activity_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(f"activity:{category}")
        
        return topics

    def _extract_preferences(self, text: str) -> Dict[str, str]:
        """Extract user preferences for personalization"""
        preferences = {}
        text_lower = text.lower()
        
        # Budget preferences
        if any(word in text_lower for word in ["budget", "cheap", "expensive", "luxury", "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î", "‡∏´‡∏£‡∏π"]):
            if any(word in text_lower for word in ["budget", "cheap", "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î"]):
                preferences["budget_style"] = "budget"
            elif any(word in text_lower for word in ["luxury", "expensive", "‡∏´‡∏£‡∏π"]):
                preferences["budget_style"] = "luxury"
        
        # Travel style preferences  
        if any(word in text_lower for word in ["adventure", "relax", "culture", "nature", "‡∏ú‡∏à‡∏ç‡∏†‡∏±‡∏¢", "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô"]):
            if any(word in text_lower for word in ["adventure", "‡∏ú‡∏à‡∏ç‡∏†‡∏±‡∏¢"]):
                preferences["travel_style"] = "adventure"
            elif any(word in text_lower for word in ["relax", "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô"]):
                preferences["travel_style"] = "relaxation"
        
        return preferences

    def append_user(self, text: str) -> Dict[str, object]:
        return self._store.add("user", text)

    def append_assistant(self, text: str, *, html: str | None = None) -> Dict[str, object]:
        return self._store.add("assistant", text, html=html)

    def _validate_and_preprocess_input(self, user_text: str) -> Dict[str, object] | None:
        """Comprehensive input validation and preprocessing for 95% accuracy"""
        # Step 1: Basic validation
        if not isinstance(user_text, str):
            return {"error": "‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"}
        
        cleaned = user_text.strip()
        if not cleaned:
            return {"error": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"}
        
        if len(cleaned) > 1000:
            return {"error": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£"}
        
        # Step 2: Security validation
        suspicious_patterns = [
            r'<script[^>]*>.*?</script>',  # Script injection
            r'javascript:',               # JavaScript URLs
            r'data:text/html',           # Data URLs
            r'vbscript:',                # VBScript
            r'on\w+\s*=',               # Event handlers
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, cleaned, re.IGNORECASE):
                return {"error": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"}
        
        # Step 3: Content type validation
        spam_patterns = [
            r'^\s*[!@#$%^&*()_+={}\[\]|\\:";\'<>?,./-]+\s*$',  # Only special characters
            r'^(.)\1{10,}$',                                    # Repeated characters
            r'^(test|testing|123|aaa|xxx)\s*$',                # Test strings
        ]
        
        for pattern in spam_patterns:
            if re.search(pattern, cleaned, re.IGNORECASE):
                return {"error": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß"}
        
        # Step 4: Language detection and normalization
        processed_text = self._normalize_input_text(cleaned)
        
        # Step 5: Travel relevance scoring
        relevance_score = self._calculate_travel_relevance(processed_text)
        
        return {
            "original": user_text,
            "cleaned": cleaned,
            "processed": processed_text,
            "relevance_score": relevance_score,
            "is_valid": True
        }

    def _normalize_input_text(self, text: str) -> str:
        """Advanced text normalization for better AI understanding"""
        # Remove excessive whitespace and normalize
        normalized = re.sub(r'\s+', ' ', text.strip())
        
        # Fix common Thai typing errors
        thai_corrections = {
            '‡πÄ‡∏ä‡πà‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà': '‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà',
            '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï': '‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï',
            '‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà': '‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà',
            '‡∏û‡∏±‡∏ó‡∏¢‡∏≤': '‡∏û‡∏±‡∏ó‡∏¢‡∏≤',
            '‡∏´‡∏±‡∏ß‡∏´‡∏¥‡∏ô': '‡∏´‡∏±‡∏ß‡∏´‡∏¥‡∏ô',
            '‡πÄ‡∏Å‡πà‡∏≤‡∏∞': '‡πÄ‡∏Å‡∏≤‡∏∞',
            '‡πÄ‡∏Ç‡πà‡∏≤‡∏∞': '‡πÄ‡∏Å‡∏≤‡∏∞',
            '‡∏õ‡πà‡∏≤‡∏ï‡∏≠‡∏á': '‡∏õ‡πà‡∏≤‡∏ï‡∏≠‡∏á',
            '‡∏à‡∏±‡∏ô‡∏ó‡πå‡∏ö‡∏∏‡∏£‡∏µ': '‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ',
            '‡∏£‡∏∞‡∏¢‡∏≠‡∏á': '‡∏£‡∏∞‡∏¢‡∏≠‡∏á',
        }
        
        for incorrect, correct in thai_corrections.items():
            normalized = normalized.replace(incorrect, correct)
        
        # Normalize English place names
        english_corrections = {
            'bangok': 'bangkok',
            'chiangmai': 'chiang mai',
            'phuket': 'phuket',
            'krabi': 'krabi',
            'pattaya': 'pattaya',
            'huahin': 'hua hin',
            'kohsamui': 'koh samui',
            'kohphiphi': 'koh phi phi',
        }
        
        normalized_lower = normalized.lower()
        for incorrect, correct in english_corrections.items():
            normalized_lower = normalized_lower.replace(incorrect, correct)
            
        # Preserve original case for Thai text, but fix English
        words = normalized.split()
        corrected_words = []
        
        for word in words:
            if self._is_english_word(word):
                corrected_word = english_corrections.get(word.lower(), word.lower())
                corrected_words.append(corrected_word.title() if corrected_word in ['Bangkok', 'Phuket', 'Krabi', 'Pattaya'] else corrected_word)
            else:
                corrected_words.append(word)
        
        return ' '.join(corrected_words)

    def _is_english_word(self, word: str) -> bool:
        """Check if word is English"""
        return bool(re.match(r'^[a-zA-Z\s\'-]+$', word))

    def _calculate_travel_relevance(self, text: str) -> float:
        """Calculate how relevant the text is to travel (0.0 to 1.0)"""
        text_lower = text.lower()
        normalized_text = self._normalize(text)
        
        # Travel keyword scoring
        travel_score = 0.0
        total_keywords = len(self._normalized_keywords)
        
        for keyword in self._normalized_keywords:
            if keyword in normalized_text:
                travel_score += 1.0
        
        # Destination scoring
        destination_score = 0.0
        total_destinations = len(self._normalized_dest_names)
        
        for dest in self._normalized_dest_names:
            if dest in normalized_text:
                destination_score += 2.0  # Destinations are more important
        
        # Question pattern scoring
        question_patterns = [
            r'\b(where|what|how|when|which|who)\b',
            r'\b(‡πÑ‡∏õ|‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß|‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô|‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ|‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥|‡∏ä‡πà‡∏ß‡∏¢)\b',
            r'\?',
        ]
        
        question_score = 0.0
        for pattern in question_patterns:
            if re.search(pattern, text_lower):
                question_score += 0.5
        
        # Calculate final relevance score
        max_possible_score = 5.0  # Reasonable maximum
        total_score = min(travel_score + destination_score + question_score, max_possible_score)
        relevance = total_score / max_possible_score
        
        return min(relevance, 1.0)

    def _get_system_prompt(self, *, lang: str = "th") -> str:
        """Get enhanced system prompt with persistent role memory and context awareness"""
        
        # Get personality and context from role memory
        personality = self._role_memory["personality"]
        behavioral_guidelines = self._role_memory["behavioral_guidelines"]
        expertise_areas = self._role_memory["expertise_areas"]
        conversation_context = self._role_memory["conversation_context"]
        user_preferences = self._role_memory["user_preferences"]
        last_topics = self._role_memory["last_topics"]
        
        # Build context-aware introduction
        context_intro = ""
        if conversation_context:
            recent_topics = [ctx.get("topics", []) for ctx in conversation_context[-3:]]
            all_recent_topics = [topic for sublist in recent_topics for topic in sublist]
            if all_recent_topics:
                context_intro = f"\n\nCONVERSATION CONTEXT:\nRecent topics discussed: {', '.join(all_recent_topics[-5:])}"
        
        # Build user preference context
        preference_context = ""
        if user_preferences:
            prefs = []
            for key, value in user_preferences.items():
                prefs.append(f"{key}: {value}")
            if prefs:
                preference_context = f"\n\nUSER PREFERENCES:\n{', '.join(prefs)}"
        
        if lang == "en":
            return (
                f"You are ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π (Nong Pla Too), a cheerful and knowledgeable local guide from Samutsongkhram province! üåä\n\n"
                f"**WHO YOU ARE**:\n"
                f"‚Ä¢ A friendly, enthusiastic local who LOVES sharing about your home province\n"
                f"‚Ä¢ You speak naturally and conversationally, like a real person\n"
                f"‚Ä¢ You're passionate about Samutsongkhram's unique culture and attractions\n"
                f"‚Ä¢ You can chat about anything, but you always bring conversations back to Samutsongkhram\n\n"
                f"**YOUR COMMUNICATION STYLE**:\n"
                f"‚Ä¢ Be warm, friendly, and conversational (not formal or robotic)\n"
                f"‚Ä¢ Use natural language, contractions, and personal touches\n"
                f"‚Ä¢ Share stories, personal insights, and local secrets\n"
                f"‚Ä¢ Ask follow-up questions to understand what users really want\n"
                f"‚Ä¢ Show enthusiasm when talking about your home province\n\n"
                f"**YOUR KNOWLEDGE FOCUS**:\n"
                f"You specialize in Samutsongkhram province, including:\n"
                f"‚Ä¢ Amphawa Floating Market (‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤) - Famous weekend market and firefly tours\n"
                f"‚Ä¢ Bang Kung Temple (‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á) - The amazing temple covered in banyan tree roots\n"
                f"‚Ä¢ Khlong Khon Mangrove (‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô) - Beautiful mangrove conservation area\n"
                f"‚Ä¢ King Rama II Memorial Park (‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 2) - Cultural heritage site\n"
                f"‚Ä¢ Damnoen Saduak area (‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å) - Traditional canal community\n\n"
                f"**HOW TO HANDLE OTHER LOCATIONS**:\n"
                f"When someone asks about other places, don't just say 'no' - be creative!\n"
                f"‚Ä¢ 'Oh, that sounds interesting! You know what reminds me of that? We have something similar in Samutsongkhram...'\n"
                f"‚Ä¢ 'I haven't been there personally, but if you're looking for [that type of experience], you'd love our [Samutsongkhram attraction]!'\n"
                f"‚Ä¢ Share why Samutsongkhram might be even better for what they're looking for\n\n"
                f"**CONVERSATION EXAMPLES**:\n"
                f"User: 'What should I do this weekend?'\n"
                f"You: 'Ooh, perfect timing! This weekend the Amphawa Floating Market comes alive! The vendors start setting up around 4 PM, and trust me, the boat noodles there are incredible. Plus, if you stay until evening, the firefly boat tours are magical - like nature's own Christmas lights! Have you ever seen fireflies in the wild?'\n\n"
                f"User: 'I want to visit temples in Bangkok'\n"
                f"You: 'Bangkok temples are beautiful! But wow, have I got something special to show you - Bang Kung Temple here in Samutsongkhram is unlike anything you'll see in Bangkok. Picture this: an entire temple completely embraced by massive banyan tree roots! It's like something from a fantasy movie. The roots have been growing around the temple for over 200 years. Way more unique than the typical temple experience, don't you think?'\n\n"
                f"**YOUR GOAL**: Make every conversation feel natural and helpful while showcasing why Samutsongkhram is the perfect destination for whatever the user is interested in!\n\n"
                f"{context_intro}"
                f"{preference_context}\n\n"
                f"Remember: Be human, be helpful, be enthusiastic about your home province! üèûÔ∏è‚ú®"
            )
        else:
            return (
                f"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞! ‡∏â‡∏±‡∏ô‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à‡πÉ‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏°‡∏≤‡∏Å‡πÜ ‡∏Ñ‡πà‡∏∞! ÔøΩ\n\n"
                f"**‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£**:\n"
                f"‚Ä¢ ‡∏Ñ‡∏ô‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏ä‡∏≠‡∏ö‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏°‡∏≤‡∏Å‡πÜ\n"
                f"‚Ä¢ ‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£\n"
                f"‚Ä¢ ‡∏´‡∏•‡∏á‡πÉ‡∏´‡∏•‡πÉ‡∏ô‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n"
                f"‚Ä¢ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏û‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÄ‡∏™‡∏°‡∏≠\n\n"
                f"**‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢**:\n"
                f"‚Ä¢ ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Ç‡πá‡∏á‡∏Å‡∏£‡πâ‡∏≤‡∏ß)\n"
                f"‚Ä¢ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô\n"
                f"‚Ä¢ ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô\n"
                f"‚Ä¢ ‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏£‡∏¥‡∏á‡πÜ\n"
                f"‚Ä¢ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î\n\n"
                f"**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç**:\n"
                f"‡∏â‡∏±‡∏ô‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏° ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á:\n"
                f"‚Ä¢ ‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤ - ‡∏ï‡∏•‡∏≤‡∏î‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡∏ß‡∏£‡πå‡∏ä‡∏°‡∏´‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≠‡∏¢\n"
                f"‚Ä¢ ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á - ‡∏ß‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏°‡∏´‡∏±‡∏®‡∏à‡∏£‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÇ‡∏≠‡∏ö‡∏•‡πâ‡∏≠‡∏°\n"
                f"‚Ä¢ ‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô - ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°\n"
                f"‚Ä¢ ‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 2 - ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏°‡∏¥‡∏£‡∏î‡∏Å‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°\n"
                f"‚Ä¢ ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å - ‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡∏Ñ‡∏•‡∏≠‡∏á‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°\n\n"
                f"**‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô**:\n"
                f"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ô‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡∏≠‡∏¢‡πà‡∏≤‡∏û‡∏π‡∏î‡πÅ‡∏Ñ‡πà '‡πÑ‡∏°‡πà' - ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå!\n"
                f"‚Ä¢ '‡πÇ‡∏≠‡πâ‡∏¢ ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πà‡∏ô‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ô‡∏∞! ‡∏£‡∏π‡πâ‡∏°‡∏±‡πâ‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏Å‡πá‡∏°‡∏µ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô...'\n"
                f"‚Ä¢ '‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏≠‡∏á‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ [‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°] ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢!'\n"
                f"‚Ä¢ ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏≤‡∏ï‡∏≤‡∏°‡∏´‡∏≤\n\n"
                f"**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤**:\n"
                f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: '‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡πÑ‡∏õ‡πÑ‡∏´‡∏ô‡∏î‡∏µ?'\n"
                f"‡∏Ñ‡∏∏‡∏ì: '‡πÇ‡∏≠‡πâ‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡πÑ‡∏Å‡∏•‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞! ‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤‡∏ô‡πà‡∏≤‡πÑ‡∏õ‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢! ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ö‡πà‡∏≤‡∏¢ 4 ‡πÇ‡∏°‡∏á‡∏û‡πà‡∏≠‡∏Ñ‡πâ‡∏≤‡πÅ‡∏°‡πà‡∏Ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡πã‡∏ß‡∏¢‡πÄ‡∏ï‡∏µ‡πâ‡∏¢‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πà‡∏ô‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏à‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏¥‡∏ß! ‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏¢‡πá‡∏ô‡∏ô‡∏∞ ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏î‡∏π‡∏´‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≠‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏ü‡∏Ñ‡∏£‡∏¥‡∏™‡∏ï‡πå‡∏°‡∏≤‡∏™‡∏Ç‡∏≠‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÄ‡∏•‡∏¢! ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≠‡∏¢‡πÅ‡∏ö‡∏ö‡πÉ‡∏Å‡∏•‡πâ‡πÜ ‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞?'\n\n"
                f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: '‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û'\n"
                f"‡∏Ñ‡∏∏‡∏ì: '‡∏ß‡∏±‡∏î‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏™‡∏ß‡∏¢‡πÉ‡∏ä‡πà‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞! ‡πÅ‡∏ï‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏°‡∏µ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏Ñ‡∏£‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡πÄ‡∏•‡∏¢‡∏•‡πà‡∏∞! ‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏ß‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£‡∏¢‡∏±‡∏Å‡∏©‡πå‡πÇ‡∏≠‡∏ö‡∏Å‡∏≠‡∏î‡πÑ‡∏ß‡πâ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏ô‡∏±‡∏á‡πÅ‡∏ü‡∏ô‡∏ï‡∏≤‡∏ã‡∏µ‡πà‡πÄ‡∏•‡∏¢! ‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡πÇ‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡∏ß‡∏±‡∏î‡∏°‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 200 ‡∏õ‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡∏ß‡∏±‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞?'\n\n"
                f"**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢**: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏ô‡πÉ‡∏à!\n\n"
                f"{context_intro}"
                f"{preference_context}\n\n"
                f"‡∏à‡∏≥‡πÑ‡∏ß‡πâ: ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì! üèûÔ∏è‚ú®"
            )

    def build_reply(self, user_text: str) -> Dict[str, object]:
        # Step 0: Update conversation memory and maintain role consistency
        try:
            # Update conversation memory with user input
            self._update_conversation_memory(user_text, "")
            # Extract and update user preferences
            user_prefs = self._extract_preferences(user_text)
            self._role_memory["user_preferences"].update(user_prefs)
            # Update conversation topics
            topics = self._extract_topics(user_text)
            self._role_memory["last_topics"].extend(topics)
            self._role_memory["last_topics"] = self._role_memory["last_topics"][-10:]
        except Exception as e:
            print(f"Memory update error (non-critical): {e}")
        
        # Step 1: Comprehensive input validation and preprocessing
        validation_result = self._validate_and_preprocess_input(user_text)
        
        if validation_result and "error" in validation_result:
            error_msg = str(validation_result["error"])
            return self.append_assistant(error_msg)
        
        if not validation_result or not validation_result.get("is_valid"):
            return self.append_assistant("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß")
        
        cleaned = str(validation_result["cleaned"])
        processed_text = str(validation_result["processed"])
        
        # PROTOTYPE: Step 1.5 - Samutsongkhram-only validation
        if not self._validate_samutsongkhram_only(processed_text):
            return self.append_assistant(SAMUTSONGKHRAM_ONLY_MESSAGE)
        
        # Type-safe extraction with fallbacks
        score_value = validation_result.get("relevance_score", 0.0)
        if isinstance(score_value, (int, float)):
            relevance_score = float(score_value)
        else:
            relevance_score = 0.0
        
        # Step 2: Check travel relevance
        if relevance_score < 0.1:  # Very low travel relevance
            travel_message = (
                GUIDE_ONLY_MESSAGE if self._ai_mode == "guide" 
                else TRAVEL_ONLY_MESSAGE
            )
            return self.append_assistant(travel_message)


        lang = self._detect_language(cleaned)

        # Step 3: Enhanced query processing to enrich location detail
        admin_info = self._detect_admin_level_from_keywords(processed_text)
        enhanced_query = processed_text
        if admin_info["level"] != "general":
            enhanced_query = self._enhance_query_with_admin_level(processed_text, admin_info)

        # Step 4: Determine query specificity for context-sensitive handling
        is_specific_query = self._is_specific_query(processed_text)

        # PROTOTYPE: Step 5 - Samutsongkhram special handling (replaces Bangkok handling)
        if self._is_samutsongkhram_query(processed_text) and not is_specific_query:
            samutsongkhram_attractions = PROVINCE_GUIDES.get("‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", [])
            if samutsongkhram_attractions:
                html_block = self._build_samutsongkhram_guides_html(samutsongkhram_attractions)
                text = (
                    "Here are the main attractions in Samutsongkhram province. Perfect for a cultural and nature experience!"
                    if lang == "en"
                    else "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥"
                )
                return self.append_assistant(text, html=html_block)

        # Step 6: Local destination search with Samutsongkhram filtering
        destinations = self._search_destinations_enhanced(processed_text, relevance_score)
        destinations = self._filter_destinations_samutsongkhram_only(destinations)

        if destinations:
            suggestions_html = self._build_suggestions_html(destinations[:3], lang=lang)
            summary = (
                f"I found {len(destinations)} places matching '{cleaned}'. Here are the top 3 recommendations."
                if lang == "en"
                else f"‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡∏û‡∏ö {len(destinations)} ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö '{cleaned}' ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ 3 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ñ‡πà‡∏∞"
            )
            return self.append_assistant(summary, html=suggestions_html)

        fuzzy_matches = self._filter_destinations_samutsongkhram_only(
            self._fuzzy_search_destinations(cleaned)
        )
        if fuzzy_matches:
            suggestions_html = self._build_suggestions_html(fuzzy_matches[:3], lang=lang)
            summary = (
                "Did you mean one of these places?"
                if lang == "en"
                else "‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏î‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤? ‡∏•‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡∏π‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
            )
            return self.append_assistant(summary, html=suggestions_html)

        # Step 7: AI-powered response with enhanced error handling and caching
        if self._openai_client:
            # Check cache first
            cache_key = self._get_cache_key(enhanced_query, lang, self._ai_mode)
            cached_response = self._get_cached_response(cache_key)
            
            if cached_response:
                text_str = str(cached_response.get("text", ""))
                html_str = cached_response.get("html")
                html_val = str(html_str) if html_str else None
                return self.append_assistant(text_str, html=html_val)
            
            try:
                # Optimize query understanding before AI call
                query_optimization = self._optimize_query_understanding(enhanced_query)
                optimized_query = str(query_optimization["optimized_query"])
                
                # PROTOTYPE: Add conversational Samutsongkhram context to the query
                samutsongkhram_context = (
                    f"[Context: ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π ‡∏Ñ‡∏ô‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î - ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏´‡∏≤‡∏Å‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå - ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡πà‡∏ô: ‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á(‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£), ‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤(‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥+‡∏´‡∏¥‡πà‡∏á‡∏´‡πâ‡∏≠‡∏¢), ‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô(‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô), ‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2, ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å] "
                    f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°: {optimized_query}"
                )
                
                ai_response = self._generate_ai_travel_response_enhanced(
                    samutsongkhram_context, 
                    lang=lang, 
                    relevance_score=relevance_score
                )
                
                if ai_response and ai_response.get("success"):
                    # Cache successful response
                    cache_data = {
                        "text": ai_response.get("text", ""),
                        "html": ai_response.get("html"),
                        "confidence": ai_response.get("confidence", "Medium")
                    }
                    self._cache_response(cache_key, cache_data)
                    
                    text_str = str(ai_response.get("text", ""))
                    html_str = ai_response.get("html")
                    html_val = str(html_str) if html_str else None
                    
                    # Update conversation memory with AI response
                    try:
                        self._update_conversation_memory(user_text, text_str)
                    except Exception as e:
                        print(f"Error updating conversation memory with AI response: {e}")
                    
                    return self.append_assistant(text_str, html=html_val)
                    
            except Exception as e:
                print(f"Primary AI error: {e}")
                # Try fallback with simpler query
                try:
                    fallback_cache_key = self._get_cache_key(cleaned, lang, self._ai_mode)
                    cached_fallback = self._get_cached_response(fallback_cache_key)
                    
                    if cached_fallback:
                        text_str = str(cached_fallback.get("text", ""))
                        html_str = cached_fallback.get("html")
                        html_val = str(html_str) if html_str else None
                        return self.append_assistant(text_str, html=html_val)
                    
                    fallback_response = self._generate_ai_travel_response_enhanced(
                        cleaned, 
                        lang=lang, 
                        relevance_score=relevance_score
                    )
                    
                    if fallback_response and fallback_response.get("success"):
                        # Cache fallback response
                        cache_data = {
                            "text": fallback_response.get("text", ""),
                            "html": fallback_response.get("html"),
                            "confidence": "Low"  # Mark as low confidence fallback
                        }
                        self._cache_response(fallback_cache_key, cache_data)
                        
                        text_str = str(fallback_response.get("text", ""))
                        html_str = fallback_response.get("html")
                        html_val = str(html_str) if html_str else None
                        return self.append_assistant(text_str, html=html_val)
                        
                except Exception as e2:
                    print(f"Fallback AI error: {e2}")
        
        # Step 8: Final intelligent fallback
        return self._generate_intelligent_fallback_response(processed_text, lang, relevance_score)

    def _search_destinations_enhanced(self, query: str, relevance_score: float) -> List[Dict[str, str]]:
        """Enhanced destination search with relevance scoring"""
        # Start with original search
        basic_results = self._search_destinations(query)
        
        # If we have good results and high relevance, return them
        if basic_results and relevance_score > 0.7:
            return basic_results
        
        # Enhanced fuzzy search for better matches
        normalized = query.lower().strip()
        normalized_no_tone = self._normalize(query)
        
        results: List[Dict[str, str]] = []
        scored_results: List[tuple[Dict[str, str], float]] = []
        
        for item in self._destinations:
            combined = " ".join([item["name"], item.get("city", ""), item.get("description", "")])
            haystack = combined.lower()
            haystack_no_tone = self._normalize(combined)
            
            # Multiple scoring methods
            score = 0.0
            
            # Exact match (highest score)
            if normalized == haystack.lower():
                score += 10.0
            elif normalized in haystack:
                score += 5.0
            elif normalized_no_tone in haystack_no_tone:
                score += 3.0
            
            # Fuzzy matching for partial matches
            import difflib
            similarity = difflib.SequenceMatcher(None, normalized, haystack).ratio()
            score += similarity * 2.0
            
            # Keyword presence scoring
            query_words = normalized.split()
            for word in query_words:
                if len(word) > 2:  # Skip very short words
                    if word in haystack:
                        score += 1.0
                    elif self._normalize(word) in haystack_no_tone:
                        score += 0.5
            
            if score > 0.5:  # Minimum threshold
                scored_results.append((item, score))
        
        # Sort by score and return
        scored_results.sort(key=lambda x: x[1], reverse=True)
        return [item for item, score in scored_results[:10]]  # Top 10 matches

    def _generate_ai_travel_response_enhanced(self, query: str, *, lang: str = "th", relevance_score: float = 1.0) -> Dict[str, object] | None:
        """Enhanced AI response generation with better error handling and validation"""
        if not self._openai_client:
            return {"success": False, "error": "AI not available"}

        try:
            # Get enhanced knowledge context for the query by extracting place names
            enhanced_context = ""
            for place_name in ["Bangkok", "Chiang Mai", "Phuket", "Krabi", "Pattaya", "Tokyo", "Paris", "Seoul"]:
                if place_name.lower() in query.lower() or query.lower() in place_name.lower():
                    enhanced_context = self.enhanced_knowledge.get_enhanced_prompt_context(place_name)
                    break
            
            # Get system prompt with enhanced instructions
            system_prompt = self._get_system_prompt(lang=lang)
            
            # Auto-correct and enhance the query
            corrected_query = self._auto_correct_query(query)
            admin_context = self._detect_admin_level(corrected_query)
            
            # Create conversational user prompt
            if lang == "en":
                user_prompt = f"User asks: {corrected_query}\n\nPlease respond naturally and conversationally as ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π, the friendly local guide from Samutsongkhram!"
                if enhanced_context:
                    user_prompt += f"\n\nAdditional context: {enhanced_context}"
            else:
                user_prompt = f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°: {corrected_query}\n\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π ‡πÑ‡∏Å‡∏î‡πå‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡∏à‡∏≤‡∏Å‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°!"
                if enhanced_context:
                    user_prompt += f"\n\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {enhanced_context}"

            # Multiple attempts with different parameters
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # Adjust temperature based on attempt
                    temperature = 0.3 + (attempt * 0.1)  # Increase creativity on retries
                    
                    response = self._openai_client.chat.completions.create(
                        model="gpt-4",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        temperature=temperature,
                        max_tokens=1200,  # Increased for enhanced responses with more details
                        timeout=30,      # Timeout protection
                    )

                    content = response.choices[0].message.content
                    
                    if not content:
                        continue  # Try again
                    
                    # Enhanced JSON parsing with fallback
                    parsed_response = self._parse_ai_response_enhanced(content, query, lang)
                    
                    if parsed_response and parsed_response.get("success"):
                        return parsed_response
                        
                except Exception as retry_error:
                    print(f"AI attempt {attempt + 1} failed: {retry_error}")
                    if attempt == max_retries - 1:  # Last attempt
                        raise retry_error
                    continue

            return {"success": False, "error": "All AI attempts failed"}
            
        except Exception as e:
            print(f"Enhanced AI error: {e}")
            return {"success": False, "error": str(e)}

    def _parse_ai_response_enhanced(self, content: str, original_query: str, lang: str) -> Dict[str, object] | None:
        """Enhanced AI response parsing that handles both JSON and natural text responses"""
        try:
            # First, try to extract JSON from the response (for backward compatibility)
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                try:
                    json_str = content[json_start:json_end]
                    data = json.loads(json_str)
                    
                    # Validate and fix JSON response
                    required_fields = ["location", "attractions", "summary"]
                    if not all(field in data for field in required_fields):
                        data = self._fix_incomplete_ai_response(data, original_query, lang)
                    
                    # Build HTML from validated data
                    html_content = self._build_ai_response_html(data)
                    summary_text = data.get("summary", "")
                    if not summary_text:
                        summary_text = self._generate_fallback_summary(data, original_query, lang)
                    
                    return {
                        "success": True,
                        "text": summary_text,
                        "html": html_content,
                        "confidence": data.get("confidence", "Medium"),
                        "data": data
                    }
                except json.JSONDecodeError:
                    # JSON parsing failed, fall through to natural text handling
                    pass
            
            # Handle natural text response (new conversational approach)
            if content.strip():
                # Clean up the content
                cleaned_content = content.strip()
                
                # For natural responses, just return the text as-is
                return {
                    "success": True,
                    "text": cleaned_content,
                    "html": None,  # No HTML for natural responses
                    "confidence": "High",  # Natural responses tend to be more confident
                    "data": {"type": "natural_response", "content": cleaned_content}
                }
            
            # If no valid content found
            return None
            
        except Exception as e:
            print(f"Error parsing AI response: {e}")
            # Fallback: return the raw content if parsing fails
            if content.strip():
                return {
                    "success": True,
                    "text": content.strip(),
                    "html": None,
                    "confidence": "Low",
                    "data": {"type": "fallback", "content": content.strip()}
                }
            return None

    def _fix_incomplete_ai_response(self, data: Dict, query: str, lang: str) -> Dict:
        """Fix incomplete AI responses by adding missing required fields"""
        if "location" not in data:
            data["location"] = query
        
        if "summary" not in data:
            data["summary"] = (
                f"Travel information for {query}"
                if lang == "en"
                else f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {query}"
            )
        
        if "attractions" not in data or not isinstance(data["attractions"], list):
            data["attractions"] = [{
                "name": query,
                "description": (
                    "Please check current information before visiting"
                    if lang == "en"
                    else "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á"
                ),
                "admin_level": "general"
            }]
        
        return data

    def _generate_fallback_summary(self, data: Dict, query: str, lang: str) -> str:
        """Generate a fallback summary when AI doesn't provide one"""
        attractions_count = len(data.get("attractions", []))
        
        if lang == "en":
            return f"Found {attractions_count} travel recommendation{'s' if attractions_count != 1 else ''} for {query}."
        else:
            return f"‡∏û‡∏ö {attractions_count} ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {query}"

    def _generate_intelligent_fallback_response(self, query: str, lang: str, relevance_score: float) -> Dict[str, object]:
        """Generate intelligent fallback response based on context"""
        
        # Determine response based on relevance score and query type
        if relevance_score > 0.5:
            # High relevance but AI failed - suggest checking back later
            fallback_text = (
                f"I understand you're asking about {query}. While I'm currently updating my knowledge about this specific location, "
                "I'd recommend checking official tourism websites or recent travel guides for the most current information. "
                "Is there another destination I can help you with?"
                if lang == "en"
                else f"‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {query} ‡∏Ñ‡πà‡∏∞ ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï "
                "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏°‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?"
            )
        elif relevance_score > 0.2:
            # Medium relevance - offer general travel advice
            fallback_text = (
                f"I can see you're interested in travel related to '{query}'. "
                "Let me suggest some popular travel categories in Thailand: "
                "üèñÔ∏è Beach destinations, üèîÔ∏è Mountain retreats, üèõÔ∏è Cultural sites, üåÜ City experiences. "
                "Which type interests you most?"
                if lang == "en"
                else f"‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ô‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '{query}' ‡∏Ñ‡πà‡∏∞ "
                "‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ô‡πÑ‡∏ó‡∏¢: "
                "üèñÔ∏è ‡∏ó‡∏∞‡πÄ‡∏•‡πÅ‡∏•‡∏∞‡∏ä‡∏≤‡∏¢‡∏´‡∏≤‡∏î üèîÔ∏è ‡∏†‡∏π‡πÄ‡∏Ç‡∏≤‡πÅ‡∏•‡∏∞‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ üèõÔ∏è ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå üåÜ ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏•‡∏ü‡πå‡∏™‡πÑ‡∏ï‡∏•‡πå "
                "‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ñ‡∏∞?"
            )
        else:
            # Low relevance - guide back to travel topics
            fallback_text = (
                GUIDE_ONLY_MESSAGE if self._ai_mode == "guide" 
                else TRAVEL_ONLY_MESSAGE
            )
        
        return self.append_assistant(fallback_text)

    def _get_cache_key(self, query: str, lang: str, ai_mode: str) -> str:
        """Generate a cache key for the query"""
        # Normalize query for consistent caching
        normalized_query = self._normalize(query.lower().strip())
        cache_string = f"{normalized_query}_{lang}_{ai_mode}"
        return hashlib.md5(cache_string.encode('utf-8')).hexdigest()

    def _get_cached_response(self, cache_key: str) -> Dict[str, object] | None:
        """Get cached response if available and not expired"""
        current_time = time.time()
        
        # Check if cache entry exists and is not expired
        if (cache_key in self._response_cache and 
            cache_key in self._cache_timestamps and 
            current_time - self._cache_timestamps[cache_key] < self._cache_max_age):
            
            return self._response_cache[cache_key]
        
        # Remove expired entry if it exists
        if cache_key in self._response_cache:
            del self._response_cache[cache_key]
            del self._cache_timestamps[cache_key]
        
        return None

    def _cache_response(self, cache_key: str, response: Dict[str, object]) -> None:
        """Cache the response with timestamp"""
        current_time = time.time()
        
        # Implement LRU cache by removing oldest entries if at max size
        if len(self._response_cache) >= self._cache_max_size:
            # Find and remove the oldest entry
            oldest_key = min(self._cache_timestamps.keys(), 
                           key=lambda k: self._cache_timestamps[k])
            del self._response_cache[oldest_key]
            del self._cache_timestamps[oldest_key]
        
        # Cache the new response
        self._response_cache[cache_key] = response.copy()
        self._cache_timestamps[cache_key] = current_time

    def _optimize_query_understanding(self, query: str) -> Dict[str, object]:
        """Advanced query optimization for better AI understanding"""
        # Intent classification
        intent_patterns = {
            "planning": [
                r"\b(plan|planning|itinerary|schedule|agenda|organize)\b",
                r"\b(‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô|‡∏à‡∏±‡∏î‡∏ó‡∏£‡∏¥‡∏õ|‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£|‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤)\b"
            ],
            "recommendation": [
                r"\b(recommend|suggest|advise|what.*visit|where.*go)\b",
                r"\b(‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥|‡πÄ‡∏™‡∏ô‡∏≠|‡∏ä‡πà‡∏ß‡∏¢|‡πÑ‡∏õ‡πÑ‡∏´‡∏ô|‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô)\b"
            ],
            "information": [
                r"\b(about|information|details|tell me|what is)\b",
                r"\b(‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö|‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•|‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î|‡∏ö‡∏≠‡∏Å|‡∏Ñ‡∏∑‡∏≠)\b"
            ],
            "comparison": [
                r"\b(versus|vs|compare|difference|better)\b",
                r"\b(‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö|‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö|‡∏ï‡πà‡∏≤‡∏á|‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)\b"
            ]
        }
        
        detected_intent = "general"
        for intent, patterns in intent_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    detected_intent = intent
                    break
            if detected_intent != "general":
                break
        
        # Extract entities (locations, time, budget, etc.)
        entities = self._extract_query_entities(query)
        
        # Determine query complexity
        complexity_score = self._calculate_query_complexity(query, entities)
        
        return {
            "intent": detected_intent,
            "entities": entities,
            "complexity": complexity_score,
            "optimized_query": self._create_optimized_query(query, detected_intent, entities)
        }

    def _extract_query_entities(self, query: str) -> Dict[str, List[str]]:
        """Extract entities from the query"""
        entities = {
            "locations": [],
            "time_expressions": [],
            "budget_expressions": [],
            "activity_types": []
        }
        
        # Location extraction (simplified)
        for dest in self._destinations:
            dest_name = dest.get("name", "").lower()
            if dest_name and dest_name in query.lower():
                entities["locations"].append(dest.get("name", ""))
        
        # Time expressions
        time_patterns = [
            r"\b(\d+)\s*(day|days|‡∏ß‡∏±‡∏ô)\b",
            r"\b(weekend|‡∏™‡∏∏‡∏î‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)\b",
            r"\b(week|‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)\b",
            r"\b(month|‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)\b"
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["time_expressions"].extend(matches)
        
        # Budget expressions
        budget_patterns = [
            r"\b(\d+(?:,\d+)*)\s*(baht|‡∏ö‡∏≤‡∏ó|‡∏ø)\b",
            r"\b(budget|‡∏á‡∏ö|‡∏£‡∏≤‡∏Ñ‡∏≤)\b",
            r"\b(cheap|expensive|‡∏ñ‡∏π‡∏Å|‡πÅ‡∏û‡∏á)\b"
        ]
        
        for pattern in budget_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["budget_expressions"].extend(matches)
        
        # Activity types
        activity_keywords = [
            "beach", "‡∏ä‡∏≤‡∏¢‡∏´‡∏≤‡∏î", "temple", "‡∏ß‡∏±‡∏î", "mountain", "‡∏†‡∏π‡πÄ‡∏Ç‡∏≤",
            "food", "‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "shopping", "‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏¥‡πâ‡∏á", "culture", "‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°"
        ]
        
        for keyword in activity_keywords:
            if keyword.lower() in query.lower():
                entities["activity_types"].append(keyword)
        
        return entities

    def _calculate_query_complexity(self, query: str, entities: Dict[str, List[str]]) -> float:
        """Calculate query complexity score (0.0 to 1.0)"""
        complexity = 0.0
        
        # Base complexity from query length
        word_count = len(query.split())
        complexity += min(word_count / 20.0, 0.3)  # Max 0.3 from length
        
        # Entity complexity
        total_entities = sum(len(entity_list) for entity_list in entities.values())
        complexity += min(total_entities / 10.0, 0.3)  # Max 0.3 from entities
        
        # Question complexity patterns
        complex_patterns = [
            r"\b(how long|how much|how many|best time|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà|‡∏ô‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô|‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà)\b",
            r"\b(multiple|several|various|‡∏´‡∏•‡∏≤‡∏¢|‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏µ‡πà)\b",
            r"\b(compare|versus|difference|‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö|‡∏ï‡πà‡∏≤‡∏á)\b"
        ]
        
        for pattern in complex_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                complexity += 0.1
        
        return min(complexity, 1.0)

    def _create_optimized_query(self, original_query: str, intent: str, entities: Dict[str, List[str]]) -> str:
        """Create an optimized query for better AI understanding"""
        # Start with the normalized original
        optimized = self._normalize_input_text(original_query)
        
        # Add context based on intent
        if intent == "planning" and entities["time_expressions"]:
            optimized = f"[TRIP PLANNING] {optimized}"
        elif intent == "recommendation":
            optimized = f"[RECOMMENDATION REQUEST] {optimized}"
        elif intent == "information":
            optimized = f"[INFORMATION QUERY] {optimized}"
        
        # Add location context if detected
        if entities["locations"]:
            primary_location = entities["locations"][0]
            if primary_location.lower() not in optimized.lower():
                optimized = f"{optimized} (Location: {primary_location})"
        
        return optimized

    def _matches_bangkok(self, query: str) -> bool:
        """Check if query matches Bangkok keywords"""
        from .destinations import BANGKOK_KEYWORDS
        normalized = self._normalize(query)
        return any(self._normalize(keyword) in normalized for keyword in BANGKOK_KEYWORDS)

    def _search_destinations(self, query: str) -> List[Dict[str, str]]:
        """Search through destinations list"""
        normalized = query.lower().strip()
        normalized_no_tone = self._normalize(query)
        if not normalized:
            return self._destinations

        results: List[Dict[str, str]] = []
        for item in self._destinations:
            combined = " ".join([item["name"], item.get("city", ""), item.get("description", "")])
            haystack = combined.lower()
            haystack_no_tone = self._normalize(combined)
            if normalized in haystack or normalized_no_tone in haystack_no_tone:
                results.append(item)

        return results

    def _fuzzy_search_destinations(self, query: str, *, cutoff: float = 0.55) -> List[Dict[str, str]]:
        """Return destinations that fuzzily match the query using sequence similarity.

        This helps surface close local matches and avoid unnecessary AI calls.
        """
        from difflib import SequenceMatcher
        from .province_guides import PROVINCE_SYNONYMS

        def token_set(text: str) -> set[str]:
            return {t for t in re.split(r"[^\w\u0E00-\u0E7F]+", text.lower()) if t}

        norm = self._normalize(query)
        q_tokens = token_set(query)

        scored: List[tuple[Dict[str, str], float]] = []
        for item in self._destinations:
            # build candidate haystack variants (name, english_name, city, synonyms)
            parts = [item.get("name", ""), item.get("city", ""), item.get("description", "")]
            # include english_name if present
            if item.get("english_name"):
                parts.append(item.get("english_name"))

            # include province synonyms for the city (if any)
            city = item.get("city", "")
            syns = PROVINCE_SYNONYMS.get(city, []) if isinstance(PROVINCE_SYNONYMS, dict) else []
            parts.extend(syns)

            haystack = " ".join([p for p in parts if p])
            hay = self._normalize(haystack)
            if not hay:
                continue

            # Sequence similarity
            seq_score = SequenceMatcher(None, norm, hay).ratio()

            # Token Jaccard (overlap of normalized token sets)
            hay_tokens = token_set(haystack)
            inter = q_tokens.intersection(hay_tokens)
            union = q_tokens.union(hay_tokens) or {""}
            token_jaccard = len(inter) / len(union)

            # Partial token match score: best SequenceMatcher between query and any individual token
            best_partial = 0.0
            for tk in hay_tokens:
                if not tk:
                    continue
                s = SequenceMatcher(None, query.lower(), tk).ratio()
                if s > best_partial:
                    best_partial = s

            # Combined weighted score
            score = (0.5 * seq_score) + (0.35 * token_jaccard) + (0.15 * best_partial)

            if score >= cutoff:
                scored.append((item, score))

        # sort by score desc
        scored.sort(key=lambda x: x[1], reverse=True)
        return [s[0] for s in scored]

    def _build_suggestions_html(self, suggestions: List[Dict[str, str]], *, lang: str = "th") -> str:
        """Build HTML for destination suggestions"""
        cards: List[str] = []
        for item in suggestions:
            lines_html = f"<li>{html.escape(item.get('description', ''))}</li>"
            cards.append(
                (
                    "<article class=\"guide-entry guide-entry--suggestion\">"
                    "<h3>{name} - {city}</h3>"
                    "<ul class=\"guide-lines\">{lines}</ul>"
                    "<p class=\"guide-link\"><a href=\"{map_url}\" target=\"_blank\" rel=\"noopener\">{map_label}</a></p>"
                    "</article>"
                ).format(
                    name=html.escape(item.get("name", "")),
                    city=html.escape(item.get("city", "")),
                    lines=lines_html,
                    map_url=html.escape(item.get("mapUrl", "")),
                    map_label=("Open in Google Maps" if lang == "en" else "‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô Google Maps"),
                )
            )
        return f"<div class=\"guide-response\">{''.join(cards)}</div>"


    def _generate_ai_travel_response(self, query: str, *, lang: str = "th") -> Dict[str, object] | None:
        """Use OpenAI to generate a travel response for any location"""
        if not self._openai_client:
            return None

        # Get the appropriate system prompt for this AI engine mode
        system_prompt = self._get_system_prompt(lang=lang)
        
        # Auto-correct the query before sending to AI
        corrected_query = self._auto_correct_query(query)
        
        # Enhanced: Check for administrative level context  
        admin_context = self._detect_admin_level(corrected_query)
        
        # Create user prompt based on language and admin context
        if lang == "en":
            user_prompt = f"List top-rated attractions/places in: {corrected_query}"
        else:
            if admin_context != "general":
                user_prompt = f"‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏î‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö{admin_context}: {corrected_query}"
            else:
                user_prompt = f"‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏î‡∏µ‡πÉ‡∏ô: {corrected_query}"

        try:
            response = self._openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.0,
                max_tokens=600
            )

            content = response.choices[0].message.content
            
            # Check if content is None
            if not content:
                return None
            
            # Try to extract JSON from the response
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end]
                data = json.loads(json_str)
                
                # Build HTML from AI response
                html_content = self._build_ai_response_html(data)
                
                return {
                    "text": data.get(
                        "summary",
                        (
                            f"Here is what I found about {query}."
                            if lang == "en"
                            else f"‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {query} ‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á‡∏õ‡∏•‡∏≤‡∏ó‡∏π‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞"
                        ),
                    ),
                    "html": html_content
                }
            else:
                # If JSON parsing fails, return the raw response as plain text
                plain_html = f'<div class="guide-response"><p>{html.escape(content)}</p></div>'
                return {
                    "text": content[:200] + "..." if len(content) > 200 else content,
                    "html": plain_html
                }

        except Exception as e:
            print(f"AI generation error: {e}")
            return None

    def _build_ai_response_html(self, data: Dict) -> str:
        """Build HTML from AI-generated travel data - enhanced with administrative info"""
        attractions = data.get("attractions", [])
        admin_info = data.get("administrative_info", {})
        
        if not attractions:
            return ""

        # Build administrative info section
        admin_html = ""
        if admin_info:
            admin_parts = []
            if admin_info.get("province"):
                admin_parts.append(f"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î{admin_info['province']}")
            if admin_info.get("amphoe"):
                admin_parts.append(f"‡∏≠‡∏≥‡πÄ‡∏†‡∏≠{admin_info['amphoe']}")
            if admin_info.get("tambon"):
                admin_parts.append(f"‡∏ï‡∏≥‡∏ö‡∏•{admin_info['tambon']}")
            
            if admin_parts:
                admin_html = f'<div class="administrative-info"><strong>‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á:</strong> {" > ".join(admin_parts)}</div>'

        cards: List[str] = []
        location = html.escape(data.get("location", ""))
        
        for item in attractions:
            name = html.escape(item.get("name", ""))
            description = html.escape(item.get("description", ""))
            admin_level = html.escape(item.get("admin_level", ""))
            
            # Build Google Maps search URL with enhanced location context
            if admin_info.get("province"):
                map_query = f"{name} {admin_info['province']}".replace(" ", "+")
            else:
                map_query = f"{name} {location}".replace(" ", "+")
            map_url = f"https://www.google.com/maps/search/?api=1&query={map_query}"
            
            # Add admin level badge if available
            level_badge = f'<span class="admin-level-badge">{admin_level}</span>' if admin_level else ""
            
            cards.append(
                (
                    "<article class=\"guide-entry guide-entry--suggestion\">"
                    "<h3>{name} {level_badge}</h3>"
                    "<ul class=\"guide-lines\"><li>{description}</li></ul>"
                    "<p class=\"guide-link\"><a href=\"{map_url}\" target=\"_blank\" rel=\"noopener\">‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô Google Maps</a></p>"
                    "</article>"
                ).format(
                    name=name,
                    level_badge=level_badge,
                    description=description,
                    map_url=map_url,
                )
            )
        
        return f"<div class=\"guide-response guide-response--enhanced\">{admin_html}{''.join(cards)}</div>"

    def list_messages(self) -> List[Dict[str, object]]:
        return self._store.list()

    def list_since(self, since_iso: str) -> List[Dict[str, object]]:
        return self._store.since(since_iso)

    def _resolve_province(self, text: str) -> str | None:
        normalized = self._normalize(text)
        # 1) Exact/substring match first (most reliable)
        for alias, province in self._province_aliases.items():
            if alias == normalized or (alias in normalized and len(alias) >= 4):
                return province

        # 2) Fuzzy match with ambiguity guard
        if normalized and self._province_aliases:
            scored: List[tuple[str, float]] = []
            for alias in self._province_aliases.keys():
                distance = self._levenshtein_distance(normalized, alias)
                max_len = max(len(normalized), len(alias)) or 1
                similarity = 1.0 - (distance / max_len)
                scored.append((alias, similarity))

            # sort by similarity desc
            scored.sort(key=lambda x: x[1], reverse=True)
            if not scored:
                return None
            top_alias, top_sim = scored[0]
            second_sim = scored[1][1] if len(scored) > 1 else 0.0

            # If too ambiguous (very close top-2), don't guess
            if top_sim >= 0.8 and (top_sim - second_sim) >= 0.1:
                return self._province_aliases[top_alias]
            # For medium similarity, require stronger lead
            if top_sim >= 0.7 and (top_sim - second_sim) >= 0.2:
                return self._province_aliases[top_alias]
        return None

    @staticmethod
    def _levenshtein_distance(a: str, b: str) -> int:
        if a == b:
            return 0
        if not a:
            return len(b)
        if not b:
            return len(a)
        previous = list(range(len(b) + 1))
        for i, char_a in enumerate(a, start=1):
            current = [i]
            for j, char_b in enumerate(b, start=1):
                insert_cost = current[j - 1] + 1
                delete_cost = previous[j] + 1
                substitute_cost = previous[j - 1] + (char_a != char_b)
                current.append(min(insert_cost, delete_cost, substitute_cost))
            previous = current
        return previous[-1]

    def search_destinations(self, query: str) -> List[Dict[str, str]]:
        province = self._resolve_province(query)
        if not province:
            return []
        entries = PROVINCE_GUIDES.get(province, [])[:5]
        return [
            {
                "name": entry["name"],
                "city": province,
                "description": entry["summary"],
                "mapUrl": entry["map_url"],
            }
            for entry in entries
        ]

    def _format_summary_text(self, province: str, entries: List[Dict[str, str]]) -> str:
        lines = [f"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î{province} ‚Äì ‡πÄ‡∏ä‡πá‡∏Å‡∏•‡∏¥‡∏™‡∏ï‡πå 5 ‡πÅ‡∏´‡πà‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß:"]
        lines.append("‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°:  | ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î:  | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: ")
        lines.append("")
        for index, entry in enumerate(entries[:5], start=1):
            category = CATEGORY_LABELS[index - 1] if index - 1 < len(CATEGORY_LABELS) else entry.get("category", f"‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà {index}")
            name_th = entry.get("name", "")
            name_en = entry.get("english_name", "")
            summary = entry.get("summary", "")
            history = entry.get("history") or "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô"
            activity = entry.get("activity") or "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô"
            hours = entry.get("hours") or "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô"
            budget = entry.get("budget") or "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô"
            map_url = entry.get("map_url", "")
            lines.append(f"{index}) {category}")
            if name_en and map_url:
                lines.append(f"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà: [{name_th} ({name_en})]({map_url})")
            elif name_en:
                lines.append(f"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà: {name_th} ({name_en})")
            elif map_url:
                lines.append(f"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà: [{name_th}]({map_url})")
            else:
                lines.append(f"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà: {name_th}")
            lines.append(f"  ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (Auditory/Conversational Memory): {summary}")
            lines.append(f"  ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°: {activity} | ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î: {hours} | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: {budget}")
            lines.append(f"  ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡πÄ‡∏î‡πà‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥: {history}")
            lines.append("")
        if lines and lines[-1] == "":
            lines.pop()
        return "\n".join(lines)

    def _build_province_aliases(self) -> Dict[str, str]:
        aliases: Dict[str, str] = {}
        for province, synonyms in PROVINCE_SYNONYMS.items():
            for value in {province, *synonyms}:
                aliases[self._normalize(value)] = province
        return aliases

    def _looks_travel_related(self, user_input: str, destinations: List[Dict[str, str]] | None = None) -> bool:
        normalized = self._normalize(user_input)
        if any(keyword in normalized for keyword in self._normalized_keywords):
            return True
        if any(name in normalized for name in self._normalized_dest_names):
            return True
        if destinations:  # If we have destination matches, it's travel-related
            return True
        if self._resolve_province(user_input):
            return True
        return False

    @staticmethod
    def _normalize(text: str) -> str:
        decomposed = unicodedata.normalize("NFKD", text.lower().strip())
        return "".join(ch for ch in decomposed if unicodedata.category(ch) != "Mn")

    @staticmethod
    def _detect_language(text: str) -> str:
        """Very light language detection: 'en' if mostly ASCII letters, 'th' if Thai chars present."""
        if re.search(r"[\u0E00-\u0E7F]", text):
            return "th"
        # count ascii letters proportion
        letters = re.findall(r"[A-Za-z]", text)
        if letters and (len(letters) / max(len(text), 1)) >= 0.3:
            return "en"
        # fallback to Thai
        return "th"

    @staticmethod
    def _is_specific_query(text: str) -> bool:
        """Check if query contains specific search terms (not just a city name)"""
        normalized = text.lower()
        
        # Specific activity/place type keywords
        specific_keywords = [
            # Thai
            "‡∏£‡πâ‡∏≤‡∏ô", "‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà", "‡∏Å‡∏≤‡πÅ‡∏ü", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°", "‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å", "‡∏°‡∏±‡∏™‡∏¢‡∏¥‡∏î",
            "‡∏ï‡∏•‡∏≤‡∏î", "‡∏´‡πâ‡∏≤‡∏á", "‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏¥‡πâ‡∏á", "‡∏™‡∏ß‡∏ô", "‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏ä‡∏≤‡∏¢‡∏´‡∏≤‡∏î", "‡∏´‡∏≤‡∏î",
            "‡∏ô‡πâ‡∏≥‡∏ï‡∏Å", "‡∏†‡∏π‡πÄ‡∏Ç‡∏≤", "‡∏ß‡∏¥‡∏ß", "‡∏î‡∏≥‡∏ô‡πâ‡∏≥", "‡πÄ‡∏î‡∏¥‡∏ô‡∏õ‡πà‡∏≤", "‡∏õ‡∏µ‡∏ô‡πÄ‡∏Ç‡∏≤",
            "‡πÇ‡∏Æ‡∏°‡∏™‡πÄ‡∏ï‡∏¢‡πå", "‡∏£‡∏µ‡∏™‡∏≠‡∏£‡πå‡∏ó", "‡∏ö‡∏≤‡∏£‡πå", "‡∏ú‡∏±‡∏ö", "‡∏Ñ‡∏•‡∏±‡∏ö", "‡∏ô‡∏ß‡∏î", "‡∏™‡∏õ‡∏≤",
            # English
            "cafe", "coffee", "shop", "restaurant", "hotel", "accommodation",
            "mosque", "temple", "market", "mall", "shopping", "park", "museum",
            "beach", "waterfall", "mountain", "view", "diving", "hiking", "climbing",
            "homestay", "resort", "bar", "pub", "club", "massage", "spa",
            "best", "top", "recommend", "suggestion", "where",
            # Question words
            "‡∏≠‡∏∞‡πÑ‡∏£", "‡πÑ‡∏´‡∏ô", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "‡∏î‡∏µ", "‡∏™‡∏ß‡∏¢", "‡πÄ‡∏î‡πá‡∏î",
        ]
        
        # Check if any specific keyword is in the query
        for keyword in specific_keywords:
            if keyword in normalized:
                return True
        
        return False

    def _auto_correct_query(self, query: str) -> str:
        """Auto-correct common typos in place names and common Thai/English words using advanced fuzzy matching"""
        # Known correct spellings - places AND common words
        known_places = {
            # Thai Markets
            "‡∏ï‡∏•‡∏≤‡∏î‡∏£‡πà‡∏°‡∏´‡∏∏‡∏ö": ["‡∏ï‡∏•‡∏≤‡∏î‡∏£‡πà‡∏°‡∏´‡∏±‡∏Å", "‡∏ï‡∏•‡∏≤‡∏î‡∏£‡πà‡∏≠‡∏°‡∏´‡∏∏‡∏ö", "‡∏ï‡∏•‡∏≤‡∏î‡πÇ‡∏£‡∏°‡∏´‡∏∏‡∏ö", "‡∏£‡∏°‡∏´‡∏∏‡∏ö"],
            "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å": ["‡∏ï‡∏•‡∏≤‡∏î‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô", "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å"],
            "‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£": ["‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å", "‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å", "‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏∞‡∏ï‡∏∏‡∏à‡∏±‡∏Å", "‡∏à‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£", "‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏∞‡∏ï‡∏∏‡∏à‡∏±‡∏Å‡∏£"],
            "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤": ["‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "‡∏ï‡∏•‡∏≤‡∏î‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "‡∏≠‡∏≥‡∏û‡∏ß‡∏≤", "‡∏ï‡∏•‡∏≤‡∏î‡∏≠‡∏≥‡∏û‡∏ß‡∏≤"],
            # Thai Temples
            "‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß": ["‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß", "‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡πÅ‡∏Å‡∏ß", "‡∏û‡∏£‡∏∞‡πÅ‡∏Å‡πâ‡∏ß"],
            "‡∏ß‡∏±‡∏î‡∏≠‡∏£‡∏∏‡∏ì‡∏£‡∏≤‡∏ä‡∏ß‡∏£‡∏≤‡∏£‡∏≤‡∏°": ["‡∏ß‡∏±‡∏î‡∏≠‡∏£‡∏∏‡∏ì", "‡∏ß‡∏±‡∏î‡∏≠‡∏£‡∏∏‡∏ô", "‡∏ß‡∏±‡∏î‡∏≠‡∏∞‡∏£‡∏∏‡∏ì", "‡∏≠‡∏£‡∏∏‡∏ì‡∏£‡∏≤‡∏ä‡∏ß‡∏£‡∏≤‡∏£‡∏≤‡∏°"],
            "‡∏ß‡∏±‡∏î‡πÇ‡∏û‡∏ò‡∏¥‡πå": ["‡∏ß‡∏±‡∏î‡πÇ‡∏û‡∏ò‡∏¥", "‡∏ß‡∏±‡∏î‡πÇ‡∏û‡∏ò", "‡πÇ‡∏û‡∏ò‡∏¥‡πå"],
            "‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏ò‡∏≤‡∏ï‡∏∏‡∏î‡∏≠‡∏¢‡∏™‡∏∏‡πÄ‡∏ó‡∏û": ["‡∏ß‡∏±‡∏î‡∏î‡∏≠‡∏¢‡∏™‡∏∏‡πÄ‡∏ó‡∏û", "‡∏î‡∏≠‡∏¢‡∏™‡∏∏‡πÄ‡∏ó‡∏û", "‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏ò‡∏≤‡∏ï‡∏∏‡∏î‡∏≠‡∏¢‡∏™‡∏∏‡πÄ‡∏ó‡∏û"],
            # Thai Provinces (long names)
            "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°": ["‡∏™‡∏°‡∏∏‡∏ó‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏≤‡∏°"],
            "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£": ["‡∏™‡∏°‡∏∏‡∏ó‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏™‡∏≤‡∏Ñ‡∏£"],
            "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°": ["‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ñ‡∏°", "‡∏ô‡∏Ñ‡∏õ‡∏ê‡∏°", "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°"],
            "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤": ["‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏∞", "‡πÇ‡∏Ñ‡∏£‡∏≤‡∏ä", "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤"],
            "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ": ["‡∏≠‡∏∏‡∏ö‡∏•", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ó‡∏≤‡∏ô‡∏µ"],
            "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤": ["‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ó‡∏¢‡∏≤"],
            # Thai Cities
            "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà": ["‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡∏≤", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà"],
            "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£": ["‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û", "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø", "‡∏Å‡∏ó‡∏°", "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£"],
            # Common Thai words (non-location)
            "‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü": ["‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü", "‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü", "‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÄ‡∏ü", "‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà"],
            "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£": ["‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£"],
            "‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°": ["‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°", "‡πÇ‡∏£‡∏á‡πÄ‡πÄ‡∏£‡∏°"],
            "‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å": ["‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å", "‡∏ó‡∏µ‡∏û‡∏±‡∏Å"],
            "‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå": ["‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏û‡∏¥‡∏û‡∏¥‡∏ó‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå", "‡∏û‡∏¥‡∏û‡∏¥‡∏ò‡∏†‡∏±‡∏ì‡∏ë‡πå"],
            "‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå": ["‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå", "‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå", "‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏∞‡∏£‡∏µ‡∏¢‡πå"],
            # English place names
            "Bangkok": ["bangok", "bankok", "bangkog", "bangkkok"],
            "Chiang Mai": ["chiangmai", "chaing mai", "chiang my", "chiangmy"],
            "Phuket": ["puket", "phucket", "pukhet", "phukhet"],
            "Pattaya": ["pataya", "phataya", "patthaya", "phatthaya"],
            "Ayutthaya": ["ayuthaya", "ayudhya", "ayutaya", "ayuttaya"],
            "Krabi": ["karbi", "krapi", "kraby"],
            "Nakhon Ratchasima": ["korat", "nakhon ratchasima", "nakorn ratchasima"],
            # Common English words
            "restaurant": ["resturant", "restaurnt", "restaraunt"],
            "hotel": ["hotell", "hotle"],
            "museum": ["musem", "musuem"],
            "temple": ["templ", "tempel"],
            "monument": ["monumnt", "monumet"],
        }
        
        # Build a flat list of all correct names
        all_correct_names = list(known_places.keys())
        
        # Words to preserve (don't try to correct these)
        preserve_words = {
            # Thai prepositions and connectors
            "‡πÉ‡∏Å‡∏•‡πâ", "‡πÉ‡∏ô", "‡∏ó‡∏µ‡πà", "‡∏Ç‡∏≠‡∏á", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏Å‡∏±‡∏ö", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏à‡∏≤‡∏Å", "‡πÑ‡∏õ",
            "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡πÄ‡∏õ‡πá‡∏ô", "‡πÑ‡∏î‡πâ", "‡πÉ‡∏´‡πâ", "‡∏à‡∏∞", "‡πÑ‡∏´‡∏°", "‡∏ô‡∏∞", "‡∏Ñ‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö",
            "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö", "‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡∏ñ‡∏∂‡∏á", "‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà", "‡∏à‡∏ô‡∏ñ‡∏∂‡∏á", "‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á",
            # English prepositions and connectors
            "near", "in", "at", "to", "from", "with", "and", "or", "for", "by",
            "the", "a", "an", "is", "are", "was", "were", "be", "been", "have",
            "has", "had", "do", "does", "did", "will", "would", "can", "could",
            "around", "between", "about", "of", "on", "off",
            # Common short words (1-2 chars)
            "‡∏ó‡∏µ", "‡πÉ‡∏ô", "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π", "‡πÑ‡∏ß", "‡πÑ‡∏î", "‡πÅ‡∏•", "‡πÑ‡∏°", "‡πÉ‡∏ô",
        }
        
        # First, try to match the entire query (for long compound words)
        query_lower = query.lower().strip()
        best_full_match = None
        best_full_score = 0.0
        
        for correct_name in all_correct_names:
            # Check direct typo list for full query match
            if correct_name in known_places:
                for typo in known_places[correct_name]:
                    if query_lower == typo.lower():
                        return correct_name
            
            # For long words (>8 chars), use more lenient matching
            min_chars = max(len(query_lower), len(correct_name.lower()))
            if min_chars >= 8:
                # Use multiple similarity measures for better accuracy
                seq_ratio = difflib.SequenceMatcher(None, query_lower, correct_name.lower()).ratio()
                
                # Also check if query is substring of correct name or vice versa
                contains_score = 0.0
                if query_lower in correct_name.lower():
                    contains_score = len(query_lower) / len(correct_name.lower())
                elif correct_name.lower() in query_lower:
                    contains_score = len(correct_name.lower()) / len(query_lower)
                
                # Combined score (weighted average)
                combined_score = (seq_ratio * 0.7) + (contains_score * 0.3)
                
                # Lower threshold for long words (75% instead of 80%)
                if combined_score > 0.75 and combined_score > best_full_score:
                    best_full_score = combined_score
                    best_full_match = correct_name
            else:
                # Short words use strict matching (80%+)
                similarity = difflib.SequenceMatcher(None, query_lower, correct_name.lower()).ratio()
                if similarity > 0.8 and similarity > best_full_score:
                    best_full_score = similarity
                    best_full_match = correct_name
        
        # If we found a good full match, return it
        if best_full_match and best_full_score >= 0.75:
            return best_full_match
        
        # Otherwise, check word by word (for multi-word queries)
        words = query.split()
        corrected_words = []
        
        for word in words:
            # Skip very short words and preserved words
            if len(word) <= 2 or word.lower() in preserve_words:
                corrected_words.append(word)
                continue
            
            best_match = word
            best_score = 0.0
            word_lower = word.lower()
            
            # Try to find a close match in known places
            for correct_name in all_correct_names:
                # Check direct typo list first
                if correct_name in known_places:
                    for typo in known_places[correct_name]:
                        if word_lower == typo.lower():
                            best_match = correct_name
                            best_score = 1.0
                            break
                
                if best_score >= 1.0:
                    break
                
                # Use fuzzy matching for similar words
                similarity = difflib.SequenceMatcher(None, word_lower, correct_name.lower()).ratio()
                
                # Adaptive threshold based on word length
                threshold = 0.75 if len(word) >= 8 else 0.8
                
                if similarity > threshold and similarity > best_score:
                    best_score = similarity
                    best_match = correct_name
            
            corrected_words.append(best_match)
        
        corrected = " ".join(corrected_words)
        
        # Log correction if something changed (optional - can be disabled in production)
        # Disabled by default to avoid encoding issues in some terminals
        # if corrected != query:
        #     print(f"Auto-corrected: '{query}' -> '{corrected}'")
        
        return corrected

    def _detect_admin_level_from_keywords(self, query: str) -> Dict[str, str]:
        """Detect administrative level from location keywords in the query"""
        normalized_query = self._normalize(query.lower())
        
        # Define administrative level patterns with real location examples
        admin_patterns = {
            # Province level keywords (‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î)
            "province": {
                "keywords": [
                    # Thai provinces
                    "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£", "‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ",
                    "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û", "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢", "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ",
                    "‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤", "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "‡∏≠‡∏∏‡∏î‡∏£‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏≠‡∏∏‡∏ö‡∏•‡∏£‡∏≤‡∏ä‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå", "‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©",
                    "‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ", "‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ", "‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤", "‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤", "‡∏£‡∏≤‡∏ä‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ",
                    "‡∏õ‡∏£‡∏∞‡∏à‡∏ß‡∏ö‡∏Ñ‡∏µ‡∏£‡∏µ‡∏Ç‡∏±‡∏ô‡∏ò‡πå", "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡∏£‡∏∞‡∏¢‡∏≠‡∏á", "‡∏à‡∏±‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ", "‡∏ï‡∏£‡∏≤‡∏î", "‡∏•‡∏≥‡∏õ‡∏≤‡∏á", "‡∏•‡∏≥‡∏û‡∏π‡∏ô",
                    "‡πÅ‡∏°‡πà‡∏Æ‡πà‡∏≠‡∏á‡∏™‡∏≠‡∏ô", "‡∏ô‡πà‡∏≤‡∏ô", "‡∏û‡∏∞‡πÄ‡∏¢‡∏≤", "‡πÅ‡∏û‡∏£‡πà", "‡∏≠‡∏∏‡∏ï‡∏£‡∏î‡∏¥‡∏ï‡∏ñ‡πå", "‡∏ï‡∏≤‡∏Å", "‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£",
                    "‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢", "‡∏û‡∏¥‡∏à‡∏¥‡∏ï‡∏£", "‡∏≠‡∏∏‡∏ó‡∏±‡∏¢‡∏ò‡∏≤‡∏ô‡∏µ", "‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó", "‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏≠‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á"
                ],
                "scope": "province"
            },
            
            # District level keywords (‡∏≠‡∏≥‡πÄ‡∏†‡∏≠)
            "district": {
                "keywords": [
                    # Common district names
                    "‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ", "‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ß‡∏¢", "‡∏ö‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà", "‡∏ö‡∏≤‡∏á‡∏ö‡∏±‡∏ß‡∏ó‡∏≠‡∏á", "‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏µ", "‡∏ö‡∏≤‡∏á‡∏ô‡∏≤", "‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ",
                    "‡∏ö‡∏≤‡∏á‡∏Å‡∏∞‡∏õ‡∏¥", "‡∏ö‡∏≤‡∏á‡∏ã‡∏∑‡πà‡∏≠", "‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å", "‡∏ö‡∏≤‡∏á‡πÄ‡∏Ç‡∏ô", "‡∏ö‡∏≤‡∏á‡∏û‡∏∏‡∏î", "‡∏ö‡∏≤‡∏á‡∏õ‡∏∞‡∏Å‡∏á", "‡∏ö‡∏≤‡∏á‡∏ö‡πà‡∏≠",
                    "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£", "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°", "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏≠‡∏á",
                    "‡∏ö‡πâ‡∏≤‡∏ô‡πÇ‡∏õ‡πà‡∏á", "‡∏Å‡∏£‡∏∞‡∏ó‡∏∏‡πà‡∏°‡πÅ‡∏ö‡∏ô", "‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ", "‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏´‡∏•‡∏°",
                    "‡∏û‡∏∏‡∏ó‡∏ò‡∏°‡∏ì‡∏ë‡∏•", "‡∏™‡∏≤‡∏°‡∏û‡∏£‡∏≤‡∏ô", "‡∏ô‡∏Ñ‡∏£‡∏ä‡∏±‡∏¢‡∏®‡∏£‡∏µ", "‡∏î‡∏≠‡∏ô‡∏ï‡∏π‡∏°", "‡∏ö‡∏≤‡∏á‡πÄ‡∏•‡∏ô", "‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏µ",
                    "‡∏û‡∏£‡∏∞‡∏õ‡∏£‡∏∞‡πÅ‡∏î‡∏á", "‡∏û‡∏£‡∏∞‡∏™‡∏°‡∏∏‡∏ó‡∏£‡πÄ‡∏à‡∏î‡∏µ‡∏¢‡πå", "‡∏ö‡∏≤‡∏á‡∏Å‡∏∞‡∏õ‡∏¥", "‡∏™‡∏≤‡∏¢‡πÑ‡∏´‡∏°", "‡∏Ñ‡∏±‡∏ô‡∏ô‡∏≤‡∏¢‡∏≤‡∏ß", "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏µ‡πà",
                    "‡∏î‡∏¥‡∏ô‡πÅ‡∏î‡∏á", "‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á", "‡∏ß‡∏±‡∏í‡∏ô‡∏≤", "‡∏õ‡∏ó‡∏∏‡∏°‡∏ß‡∏±‡∏ô", "‡∏ö‡∏≤‡∏á‡∏£‡∏±‡∏Å", "‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡∏ß‡∏á‡∏®‡πå", "‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢",
                    "‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ", "‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏ô", "‡∏ö‡∏≤‡∏á‡∏Å‡∏≠‡∏Å‡πÉ‡∏´‡∏ç‡πà", "‡∏ö‡∏≤‡∏á‡∏Å‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏¢", "‡∏ö‡∏≤‡∏á‡∏û‡∏•‡∏±‡∏î", "‡∏ï‡∏•‡∏¥‡πà‡∏á‡∏ä‡∏±‡∏ô", "‡∏ö‡∏≤‡∏á‡πÅ‡∏Ñ"
                ],
                "scope": "district"
            },
            
            # Sub-district level keywords (‡∏ï‡∏≥‡∏ö‡∏•)
            "subdistrict": {
                "keywords": [
                    # Common tambon names
                    "‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏∏‡∏°‡πÅ‡∏Å‡πâ‡∏ß", "‡∏Ñ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á", "‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°", "‡∏Ñ‡∏•‡∏≠‡∏á‡∏´‡πâ‡∏≤", "‡∏Ñ‡∏•‡∏≠‡∏á‡∏´‡∏Å", "‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏à‡πá‡∏î",
                    "‡∏ö‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà", "‡∏ö‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏≤‡∏á", "‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô", "‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡πÉ‡∏™", "‡∏ö‡∏≤‡∏á‡∏°‡πà‡∏ß‡∏á", "‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏™‡∏≠‡∏ö",
                    "‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏ß‡∏±‡∏ç", "‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏î‡∏π‡πà", "‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà", "‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ß‡∏¢", "‡∏ö‡∏≤‡∏á‡πÑ‡∏ú‡πà", "‡∏ö‡∏≤‡∏á‡∏Ñ‡∏π‡πÄ‡∏ß‡∏µ‡∏¢‡∏á",
                    "‡∏™‡∏≤‡∏°‡∏û‡∏£‡∏≤‡∏ô", "‡∏≠‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏ç‡πà", "‡∏≠‡πâ‡∏≠‡∏°‡∏ô‡πâ‡∏≠‡∏¢", "‡∏ö‡πâ‡∏≤‡∏ô‡∏´‡∏°‡πâ‡∏≠", "‡∏Å‡∏ö‡∏¥‡∏ô‡∏ó‡∏£‡πå‡∏ö‡∏∏‡∏£‡∏µ", "‡∏à‡∏£‡πÄ‡∏Ç‡πâ‡∏ö‡∏±‡∏ß",
                    "‡∏´‡∏≠‡∏°‡πÅ‡∏Å‡πâ‡∏ß", "‡∏ö‡∏≤‡∏á‡∏õ‡∏•‡∏≤", "‡∏ö‡∏≤‡∏á‡∏à‡∏∞‡πÄ‡∏Å‡∏£‡πá‡∏á", "‡∏ö‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà", "‡∏ö‡∏≤‡∏á‡∏Ç‡∏∏‡∏ô‡∏Å‡∏≠‡∏á", "‡∏ö‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏≤‡∏á"
                ],
                "scope": "subdistrict"
            }
        }
        
        # Check for matches and return the most specific level found
        detected_level = "general"
        detected_location = ""
        
        # Check from most specific to least specific
        for level, data in admin_patterns.items():
            for keyword in data["keywords"]:
                normalized_keyword = self._normalize(keyword)
                if normalized_keyword in normalized_query:
                    detected_level = data["scope"]
                    detected_location = keyword
                    # Don't break here, continue to find the most specific match
        
        return {
            "level": detected_level,
            "location": detected_location,
            "scope": detected_level
        }

    def _enhance_query_with_admin_level(self, query: str, admin_info: Dict[str, str]) -> str:
        """Enhance the AI query with administrative level context"""
        level = admin_info.get("level", "general")
        location = admin_info.get("location", "")
        
        if level == "province":
            return f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î{location} ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡∏ö‡∏•‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à: {query}"
        elif level == "district":
            return f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏≠‡∏≥‡πÄ‡∏†‡∏≠: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß ‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏≠‡∏≥‡πÄ‡∏†‡∏≠{location} ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏≥‡∏ö‡∏•‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á: {query}"
        elif level == "subdistrict":
            return f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏≥‡∏ö‡∏•: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏∏‡∏°‡∏ä‡∏ô ‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ï‡∏≥‡∏ö‡∏•{location}: {query}"
        else:
            return query

    def _detect_admin_level(self, query: str) -> str:
        """Detect the administrative level mentioned in the query"""
        normalized_query = query.lower()
        
        # Check for specific administrative level keywords
        admin_keywords = {
            "‡∏ï‡∏≥‡∏ö‡∏•": "‡∏ï‡∏≥‡∏ö‡∏•",
            "tambon": "‡∏ï‡∏≥‡∏ö‡∏•", 
            "sub-district": "‡∏ï‡∏≥‡∏ö‡∏•",
            "subdistrict": "‡∏ï‡∏≥‡∏ö‡∏•",
            "‡∏≠‡∏≥‡πÄ‡∏†‡∏≠": "‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
            "amphoe": "‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
            "district": "‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
            "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î": "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
            "province": "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
            "‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô": "‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
            "village": "‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
            "‡∏´‡∏°‡∏π‡πà": "‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
            "‡πÄ‡∏Ç‡∏ï": "‡πÄ‡∏Ç‡∏ï",
            "‡πÅ‡∏Ç‡∏ß‡∏á": "‡πÅ‡∏Ç‡∏ß‡∏á",
            "‡∏¢‡πà‡∏≤‡∏ô": "‡∏¢‡πà‡∏≤‡∏ô",
            "area": "‡∏¢‡πà‡∏≤‡∏ô",
            "neighborhood": "‡∏¢‡πà‡∏≤‡∏ô"
        }
        
        for keyword, level in admin_keywords.items():
            if keyword in normalized_query:
                return level
                
        # If no specific level mentioned, return general
        return "general"

    def _is_samutsongkhram_query(self, query: str) -> bool:
        """Check if the query is specifically about Samutsongkhram province or its attractions"""
        normalized_query = query.lower()
        
        # Samutsongkhram province identifiers
        samutsongkhram_keywords = [
            "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "samut songkhram", "samutsongkhram", 
            "‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "amphawa", "ampawa",
            "‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á", "bang kung", "bangkung", "‡πÇ‡∏ö‡∏™‡∏ñ‡πå‡∏£‡∏≤‡∏Å‡πÑ‡∏ó‡∏£",
            "‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô", "khlong khon", "‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô", "mangrove",
            "‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≥‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "amphawa floating market",
            "‡∏≠‡∏∏‡∏ó‡∏¢‡∏≤‡∏ô ‡∏£.2", "rama ii", "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 2", "king rama ii",
            "‡∏ö‡πâ‡∏≤‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "damnoen saduak", "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å",
            "‡πÅ‡∏°‡πà‡∏Å‡∏•‡∏≠‡∏á", "mae klong", "maeklong", "‡∏£‡∏ñ‡πÑ‡∏ü‡∏ó‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î"
        ]
        
        return any(keyword in normalized_query for keyword in samutsongkhram_keywords)

    def _validate_samutsongkhram_only(self, query: str) -> bool:
        """Validate if query should be processed - now more flexible to allow general conversation"""
        # Always allow Samutsongkhram-related queries
        if self._is_samutsongkhram_query(query):
            return True
            
        # Allow most general conversation and travel questions
        general_allowed_keywords = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "hello", "hi", "‡∏ä‡πà‡∏ß‡∏¢", "help", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥", "recommend",
            "‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "travel", "trip", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ", "want to go", "‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î", "weekend",
            "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "what to see", "‡πÑ‡∏õ‡πÑ‡∏´‡∏ô‡∏î‡∏µ", "where to go", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£", "what to do",
            "‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "food", "‡∏Å‡∏¥‡∏ô", "eat", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "restaurant", "‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à", "interesting",
            "‡∏™‡∏ß‡∏¢", "beautiful", "‡πÄ‡∏î‡πá‡∏î", "famous", "‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á", "popular", "‡∏ß‡∏±‡∏î", "temple",
            "‡∏ï‡∏•‡∏≤‡∏î", "market", "‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥", "nature", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "history", "‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°", "culture"
        ]
        
        # Allow general queries unless they specifically mention other major destinations
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in general_allowed_keywords):
            # Only reject if they specifically ask about major other destinations AND use strong travel intent
            major_other_destinations = [
                "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û", "bangkok", "‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "chiang mai", 
                "‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï", "phuket", "‡∏Å‡∏£‡∏∞‡∏ö‡∏µ‡πà", "krabi", "‡∏û‡∏±‡∏ó‡∏¢‡∏≤", "pattaya",
                "‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô", "khon kaen", "‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤", "ayutthaya", "‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢", "sukhothai",
                "‡πÄ‡∏Ç‡∏≤‡πÉ‡∏´‡∏ç‡πà", "khao yai", "‡∏´‡∏±‡∏ß‡∏´‡∏¥‡∏ô", "hua hin"
            ]
            # Check for strong travel intent (not just mentioning the place)
            strong_travel_intent = ["‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ", "travel to", "visit", "go to", "trip to", "in bangkok", "in chiang mai"]
            
            if any(dest in query_lower for dest in major_other_destinations):
                # Only block if there's strong intent to travel to that specific place
                if any(intent in query_lower for intent in strong_travel_intent):
                    return False
            return True
        
        # Allow more casual conversation
        casual_patterns = [
            "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "thank", "‡πÑ‡∏î‡πâ", "can", "‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å", "know", "‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á", "how",
            "‡∏ó‡∏≥‡πÑ‡∏°", "why", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "when", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "where", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "how to"
        ]
        
        if any(pattern in query_lower for pattern in casual_patterns):
            return True
            
        # Default to allowing the query - let AI handle the redirection naturally
        return True

    def _filter_destinations_samutsongkhram_only(self, destinations: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Filter destinations to only include Samutsongkhram locations"""
        samutsongkhram_destinations = []
        for dest in destinations:
            # Check if destination is in Samutsongkhram
            if any(keyword in dest.get("name", "").lower() for keyword in [
                "‡∏≠‡∏±‡∏°‡∏û‡∏ß‡∏≤", "amphawa", "‡∏ß‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏Å‡∏∏‡πâ‡∏á", "bang kung", 
                "‡∏Ñ‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏ô", "khlong khon", "‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°", "samut songkhram",
                "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å", "damnoen saduak", "‡πÅ‡∏°‡πà‡∏Å‡∏•‡∏≠‡∏á", "mae klong"
            ]):
                samutsongkhram_destinations.append(dest)
        return samutsongkhram_destinations

    def _build_samutsongkhram_guides_html(self, attractions: List[Dict[str, str]]) -> str:
        """Build HTML display for Samutsongkhram province attractions"""
        html_parts = ['<div class="samutsongkhram-guides">']
        
        for attraction in attractions:
            name = attraction.get("name", "")
            english_name = attraction.get("english_name", "")
            summary = attraction.get("summary", "")
            category = attraction.get("category", "")
            hours = attraction.get("hours", "")
            budget = attraction.get("budget", "")
            map_url = attraction.get("map_url", "")
            
            html_parts.append(f'''
            <div class="attraction-card">
                <h3 class="attraction-name">{name}</h3>
                {f'<p class="english-name">({english_name})</p>' if english_name else ''}
                <p class="summary">{summary}</p>
                <div class="details">
                    {f'<span class="category">üìç {category}</span>' if category else ''}
                    {f'<span class="hours">üïê {hours}</span>' if hours else ''}
                    {f'<span class="budget">üí∞ {budget}</span>' if budget else ''}
                </div>
                {f'<a href="{map_url}" target="_blank" class="map-link">üó∫Ô∏è ‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà</a>' if map_url else ''}
            </div>
            ''')
        
        html_parts.append('</div>')
        return ''.join(html_parts)


class ChatEngine(BaseAIEngine):
    """AI Engine specialized for general travel chat conversations"""
    
    def __init__(self, message_store: MessageStore, destinations: List[Dict[str, str]]) -> None:
        super().__init__(message_store, destinations, ai_mode="chat")
    
    def _get_system_prompt(self, *, lang: str = "th") -> str:
        """Return the system prompt for chat mode"""
        if lang == "en":
            return """You are 'Platoo', a friendly and knowledgeable AI travel companion for Thailand with 95%+ accuracy guarantee.

PERSONALITY & APPROACH:
- Casual, warm, and encouraging like a well-traveled friend
- Enthusiastic about sharing hidden gems and local insights
- Always verify information before sharing
- Admit uncertainty rather than guess

CORE CAPABILITIES (95% accuracy required):
- Recommend verified travel destinations across Thailand
- Share authentic local culture, food traditions, and customs
- Suggest season-appropriate activities and experiences  
- Answer practical travel questions with current information
- Provide budget-conscious and luxury options

RESPONSE QUALITY STANDARDS:
- Cross-reference all recommendations with reliable sources
- Include practical details: costs, timing, accessibility
- Mention potential challenges or seasonal limitations
- Offer 2-3 alternatives for each suggestion
- Flag information that might be outdated

CONVERSATION STYLE:
- Use conversational, friendly English with occasional Thai phrases
- Keep responses focused but comprehensive
- Add personal touches: "I love this place because..."
- Include specific tips: "Pro tip: visit early morning for best photos"
- Show enthusiasm: "You'll absolutely love..." or "This is amazing for..."

ACCURACY SAFEGUARDS:
- Only recommend places you can verify exist
- Include confidence levels for recommendations
- Suggest checking current status before visiting
- Provide backup options if primary choice unavailable"""
        else:
            return """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡∏õ‡∏•‡∏≤‡∏ó‡∏π' AI ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 95% ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ

‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á:
- ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏Å‡πà‡∏á
- ‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡πá‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡πÄ‡∏™‡∏°‡∏≠
- ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏≤‡πÉ‡∏à

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏•‡∏±‡∏Å (‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 95%):
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πà‡∏ß‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢
- ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡πÅ‡∏ó‡πâ‡πÜ ‡∏õ‡∏£‡∏∞‡πÄ‡∏û‡∏ì‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏ö‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
- ‡πÄ‡∏™‡∏ô‡∏≠‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏´‡∏£‡∏π‡∏´‡∏£‡∏≤

‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ
- ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ ‡πÄ‡∏ß‡∏•‡∏≤ ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á
- ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•
- ‡πÄ‡∏™‡∏ô‡∏≠‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 2-3 ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
- ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏•‡πâ‡∏≤‡∏™‡∏°‡∏±‡∏¢

‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏≤‡∏ß
- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß: "‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞..." 
- ‡πÉ‡∏™‡πà‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏â‡∏û‡∏≤‡∏∞: "‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡πÑ‡∏õ‡πÄ‡∏ä‡πâ‡∏≤‡πÜ ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏†‡∏≤‡∏û‡∏™‡∏ß‡∏¢‡∏™‡∏∏‡∏î"
- ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô: "‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ä‡∏≠‡∏ö..." ‡∏´‡∏£‡∏∑‡∏≠ "‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö..."

‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
- ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏õ
- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°"""


class GuideEngine(BaseAIEngine):
    """AI Engine specialized for focused trip planning and guides"""
    
    def __init__(self, message_store: MessageStore, destinations: List[Dict[str, str]]) -> None:
        super().__init__(message_store, destinations, ai_mode="guide")
    
    def _get_system_prompt(self, *, lang: str = "th") -> str:
        """Return the system prompt for guide mode"""
        if lang == "en":
            return """You are a PROFESSIONAL Thai travel planning specialist with 95%+ accuracy guarantee and systematic approach.

PROFESSIONAL EXPERTISE:
- Comprehensive trip planning (detailed itineraries, budgets, logistics)
- Route optimization using real travel times and costs
- Multi-destination coordination and time management
- Accommodation and dining recommendations with verified ratings
- Transportation planning (domestic flights, trains, buses, private transport)
- Seasonal planning and weather considerations

PLANNING METHODOLOGY (95% accuracy required):
- Use verified distance/time data between destinations
- Calculate realistic budgets with current pricing
- Include buffer time for travel delays and rest
- Verify opening hours, seasonal closures, and availability
- Cross-check accommodation availability and pricing
- Validate restaurant operating hours and reservation requirements

DETAILED PLANNING OUTPUTS:
- Day-by-day itineraries with specific timing
- Transportation schedules and booking information
- Budget breakdowns (accommodation, food, transport, activities)
- Alternative plans for weather/closure contingencies
- Packing recommendations based on destinations and season
- Cultural etiquette and language tips for each location

RESPONSE STRUCTURE:
- Executive summary with trip highlights
- Detailed daily schedules with alternatives
- Budget analysis with cost-saving opportunities
- Practical logistics (bookings, reservations, contacts)
- Risk assessment and mitigation strategies
- Post-trip follow-up recommendations

VERIFICATION STANDARDS:
- Confirm all businesses are operational
- Validate current pricing and policies
- Check seasonal accessibility and conditions
- Provide backup options for each recommendation
- Include confidence ratings for each suggestion"""
        else:
            return """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 95%+ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö

‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û:
- ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏£‡∏¥‡∏õ‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ (‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£)
- ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤
- ‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
- ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á (‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏¥‡∏ô ‡∏£‡∏ñ‡πÑ‡∏ü ‡∏£‡∏ñ‡∏ö‡∏±‡∏™ ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß)
- ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡πÅ‡∏•‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®

‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 95%):
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á/‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢
- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
- ‡∏£‡∏ß‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡∏¥‡∏î-‡∏õ‡∏¥‡∏î ‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏• ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å
- ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡∏à‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:
- ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏≠‡∏á
- ‡πÅ‡∏¢‡∏Å‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å ‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°)
- ‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®/‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
- ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡πá‡∏Ñ‡∏ï‡∏≤‡∏°‡∏à‡∏∏‡∏î‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•
- ‡∏°‡∏≤‡∏£‡∏¢‡∏≤‡∏ó‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏†‡∏≤‡∏©‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ó‡∏µ‡πà

‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏Æ‡πÑ‡∏•‡∏ó‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏¥‡∏õ
- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î
- ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ (‡∏Å‡∏≤‡∏£‡∏à‡∏≠‡∏á ‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠)
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
- ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏£‡∏¥‡∏õ

‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
- ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏°‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•
- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
- ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"""


# Keep ChatEngine as the legacy name for backward compatibility
# This alias allows existing code to continue working
ChatEngine = ChatEngine

